{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as skl    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def My_preprocesing(url):\n",
    "    dataset_df = pd.read_csv(url)\n",
    "    # Wyrzucanie niepotrzbnych rzeczy \n",
    "    dataset_df = dataset_df.drop([\"zipcode\",\"thumbnail_url\",\"id\",\"neighbourhood\",\"description\",\"name\",\"host_response_rate\"],axis = 1)\n",
    "    \n",
    "    change_names = dataset_df.property_type.value_counts().index.tolist()[5:]\n",
    "    for index,data in dataset_df.iterrows():\n",
    "        if data[1] in change_names:\n",
    "            dataset_df.property_type[index] = 'Other'\n",
    "    \n",
    "    #one hot encoding\n",
    "    ammenities_set = set()\n",
    "    for index,data in dataset_df.iterrows():\n",
    "        temp = data[3].replace('{', ',')\n",
    "        temp1 =temp.replace('}', ',')\n",
    "        temp2 =  re.findall('(?<=,)[^,]+(?=,)', temp1)\n",
    "        for i in temp2:\n",
    "            ammenities_set.add(i)\n",
    "    z = np.zeros((dataset_df.shape[0],len(ammenities_set)))\n",
    "    ammenities_df = pd.DataFrame(z,columns=ammenities_set)\n",
    "    arr =ammenities_df.values\n",
    "    ammenities_list = list(ammenities_set)\n",
    "    for index,data in dataset_df.iterrows():\n",
    "        temp = data[3].replace('{', ',')\n",
    "        temp =temp.replace('}', ',')\n",
    "        temp =  re.findall('(?<=,)[^,]+(?=,)', temp)\n",
    "        for fraze in temp:\n",
    "            arr[index,ammenities_list.index(fraze)] = 1\n",
    "    ammenities_df = pd.DataFrame(arr,columns=ammenities_set)\n",
    "    suma = []\n",
    "    for i in ammenities_list:\n",
    "        suma.append(ammenities_df[i].sum())\n",
    "    get_rid = []\n",
    "    for i in range(len(suma)):\n",
    "        if suma[i] < 10000: # patrząc po wykresie sumy wydawało się to dobrym pomysłem\n",
    "            get_rid.append(ammenities_list[i])\n",
    "    ammenities_df=  ammenities_df.drop(get_rid,axis=1)\n",
    "    \n",
    "    \n",
    "    def encoding(lablename,data):\n",
    "        le = LabelEncoder()\n",
    "        le.fit( data[lablename])\n",
    "        data[lablename]= le.transform(data[lablename].values)\n",
    "        return data\n",
    "    \n",
    "    arr = dataset_df.bed_type.values\n",
    "    arr[arr != 'Real Bed'] = 'Other'\n",
    "    dataset_df.bed_type = arr\n",
    "    col_enc = [\"room_type\",\"city\",\"bed_type\",\"host_has_profile_pic\", \"host_identity_verified\",\"instant_bookable\",\"cancellation_policy\",\"cleaning_fee\",\"property_type\"]\n",
    "    for i in col_enc:\n",
    "        dataset_df = encoding(i,dataset_df)\n",
    "    dataset_df= dataset_df.drop([\"amenities\"],axis=1)\n",
    "    \n",
    "    \n",
    "        \n",
    "    def time_coding(columns,dataset_df):\n",
    "        for i in columns:\n",
    "            dataset_df[i] = dataset_df[i].astype('datetime64[ns]')\n",
    "            min = dataset_df[i].min()\n",
    "            temp = []\n",
    "            for idx in range(dataset_df.shape[0]):\n",
    "                if math.isnan((dataset_df[i][idx] - min).days):\n",
    "                    temp.append(0)\n",
    "                else:\n",
    "                    temp.append(int((dataset_df[i][idx] - min).days))\n",
    "            dataset_df[i] = pd.DataFrame(temp)\n",
    "            \n",
    "            \n",
    "        return dataset_df\n",
    "    \n",
    "    dataset_df =time_coding([\"host_since\",\"last_review\",\"first_review\"],dataset_df)\n",
    "    \n",
    "    for i in [\"host_since\",\"last_review\",\"first_review\"]:\n",
    "        n = i\n",
    "        scaler = skl.preprocessing.StandardScaler()\n",
    "        scaler.fit(dataset_df[n].values.reshape(-1,1))\n",
    "        dataset_df[n] = scaler.transform(dataset_df[n].values.reshape(-1,1)) \n",
    "        \n",
    "    return pd.concat([dataset_df, ammenities_df], axis=1, join='inner').dropna()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macie\\AppData\\Local\\Temp/ipykernel_18296/92403983.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_df.property_type[index] = 'Other'\n"
     ]
    }
   ],
   "source": [
    "data = My_preprocesing(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y,X = data['log_price'].values,data.drop(['log_price'],axis=1).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge1 = skl.linear_model.Ridge()\n",
    "Ridge2 = skl.linear_model.Ridge()\n",
    "Ridge3 = skl.linear_model.Ridge()\n",
    "GBR = GradientBoostingRegressor()\n",
    "KNN = KNeighborsRegressor()\n",
    "svr = SVR()\n",
    "RFR = RandomForestRegressor(n_estimators = 150, random_state = 0)\n",
    "RFR2 = RandomForestRegressor(n_estimators = 150, random_state = 0)\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from S_regresor import Stacker_Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stacker = Stacker_Regresion([Ridge1,Ridge2,Ridge3],Final_model=RFR2,Boosting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model, name: RandomForestRegressor(n_estimators=150, random_state=0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7656, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18296/810182455.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy_stacker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodels_to_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Nauka\\Polibuda\\V Semestr\\ML\\Kontaminacja\\S_regresor.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, models_to_train, random_state)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mStacker_Regresion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBoosting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mF_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mStacker_Regresion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFinal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mStacker_Regresion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFinal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    327\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    570\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    332\u001b[0m             \u001b[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7656, 3]"
     ]
    }
   ],
   "source": [
    "my_stacker.fit(X=X_train,Y=Y_train,models_to_train=[True,True,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Final model: \n",
      "\n",
      "Median absolute error:  0.19852481843434822\n",
      "Median absolute % error:  0.054752114150339884\n",
      "Mean sq log error:  0.0036662535772049106\n",
      "Mean sq error:  0.12086026785039888\n",
      "r2 score:  0.7309646308297257\n",
      "\n",
      "Evaluating model: Ridge() \n",
      "\n",
      "Median absolute error:  0.26179768108823875\n",
      "Median absolute % error:  0.06883065662412988\n",
      "Mean sq log error:  0.005357385085777867\n",
      "Mean sq error:  0.17389728860224682\n",
      "r2 score:  0.61290404142803\n",
      "\n",
      "Evaluating model: GradientBoostingRegressor() \n",
      "\n",
      "Median absolute error:  0.21036705898108998\n",
      "Median absolute % error:  0.057054744332398856\n",
      "Mean sq log error:  0.0038957014661290664\n",
      "Mean sq error:  0.12709656967149852\n",
      "r2 score:  0.7170826016687979\n",
      "\n",
      "Evaluating model: SVR() \n",
      "\n",
      "Median absolute error:  0.26930173317757067\n",
      "Median absolute % error:  0.07097079724799701\n",
      "Mean sq log error:  0.005687860938383858\n",
      "Mean sq error:  0.18563502129676046\n",
      "r2 score:  0.5867758083465074\n",
      "\n",
      "Evaluating model: RandomForestRegressor(n_estimators=150, random_state=0) \n",
      "\n",
      "Median absolute error:  0.19588344855465056\n",
      "Median absolute % error:  0.05403471614373285\n",
      "Mean sq log error:  0.0035907919348769765\n",
      "Mean sq error:  0.1183754575149774\n",
      "r2 score:  0.7364958271260611\n"
     ]
    }
   ],
   "source": [
    "my_stacker.evaluate_models(X=X_test,Y=Y_test) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aad7386399a7c27607516c50e72e17040ab97cac8def3ed7dd10a0a9a87ff1df"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
